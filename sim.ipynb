{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d4119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from models.encoder import Encoder\n",
    "\n",
    "import utils\n",
    "\n",
    "import argparse,random,logging,numpy,os\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "\n",
    "from time import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from util import load_dataset, make_iter, Params\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f0531d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/train_ex.json\") as f:\n",
    "    examples = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ceed73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(columns=[\"doc\", \"sent\"])\n",
    "\n",
    "count = 0\n",
    "\n",
    "for n, ex in enumerate(examples[:100]):\n",
    "    sents = ex['doc'].split('</s> ')\n",
    "    sents = [x[3:].replace(\"</s>\", \"\") for x in sents]\n",
    "    inputs = ex['input'][3:-4]\n",
    "\n",
    "    tmp = list()\n",
    "    tmp.append(ex['doc'])\n",
    "    tmp.append(sents)\n",
    "\n",
    "    train_df.loc[count] = tmp\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e5ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params('config/params.json')\n",
    "\n",
    "embed = torch.Tensor(np.load(\"./data/embedding.npz\")['embedding'])\n",
    "with open(\"./data/word2id.json\") as f:\n",
    "    word2id = json.load(f)\n",
    "vocab = utils.Vocab(embed, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9397f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (token_embedding): Embedding(153824, 256, padding_idx=1)\n",
       "  (pos_embedding): Embedding(1025, 256)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): EncoderLayer(\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (attentions): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (o_w): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (position_wise_ffn): PositionWiseFeedForward(\n",
       "        (conv1): Conv1d(256, 2048, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): EncoderLayer(\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (attentions): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (o_w): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (position_wise_ffn): PositionWiseFeedForward(\n",
       "        (conv1): Conv1d(256, 2048, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): EncoderLayer(\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (attentions): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (o_w): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (position_wise_ffn): PositionWiseFeedForward(\n",
       "        (conv1): Conv1d(256, 2048, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): EncoderLayer(\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (attentions): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (o_w): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (position_wise_ffn): PositionWiseFeedForward(\n",
       "        (conv1): Conv1d(256, 2048, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): EncoderLayer(\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (attentions): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (o_w): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (position_wise_ffn): PositionWiseFeedForward(\n",
       "        (conv1): Conv1d(256, 2048, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): EncoderLayer(\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (attentions): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (q_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (k_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (v_w): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (o_w): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (position_wise_ffn): PositionWiseFeedForward(\n",
       "        (conv1): Conv1d(256, 2048, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ae\n",
    "\"\"\"\n",
    "# load\n",
    "pretrained_dict = torch.load(\"./entire_ae.pt\")\n",
    "model = Encoder(params)\n",
    "model_dict = model.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict)\n",
    "# 3. load the new state dict\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(params.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98bef34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ae embedding 만들기\n",
    "\"\"\"\n",
    "doc_num = len(examples)\n",
    "time_cost = 0\n",
    "file_id = 1\n",
    "\n",
    "sent_trunc=100\n",
    "word_trunc=100\n",
    "PAD_TOKEN = '<pad>'\n",
    "PAD_IDX = 0\n",
    "UNK_TOKEN = '<unk>'\n",
    "UNK_IDX = 1\n",
    "\n",
    "def w2i(w):\n",
    "    if w in word2id:\n",
    "        return word2id[w]\n",
    "    else:\n",
    "        return UNK_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e29ea95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_output = list()\n",
    "sent_output = list()\n",
    "\n",
    "\n",
    "for sents in train_df['sent']:\n",
    "    \"\"\"\n",
    "    doc\n",
    "    \"\"\"\n",
    "    # trunc or pad sent\n",
    "    max_sent_len = 0\n",
    "    batch_sents = []\n",
    "    for sent in sents:\n",
    "        words = sent.split()\n",
    "        if len(words) > word_trunc:\n",
    "            words = words[:word_trunc]\n",
    "        batch_sents.append(words)\n",
    "\n",
    "    features = []\n",
    "    for sent in batch_sents:\n",
    "        feature = [w2i(w) for w in sent] + [PAD_IDX for _ in range(word_trunc-len(sent))]\n",
    "        feature = np.array(feature)\n",
    "        features.append(feature)\n",
    "\n",
    "    sent_embedding = features\n",
    "    \n",
    "    \"\"\"\n",
    "    input\n",
    "\n",
    "    # trunc or pad sent\n",
    "    words = inputs.split()\n",
    "    if len(words) > word_trunc:\n",
    "        words = words[:word_trunc]\n",
    "    \n",
    "    input_feature = [[w2i(w) for w in words] + [PAD_IDX for _ in range(word_trunc-len(words))]]\n",
    "    \"\"\"\n",
    "\n",
    "    features = torch.LongTensor(features)\n",
    "    features = Variable(features)\n",
    "    \n",
    "    \n",
    "    encoder_output = model(features)\n",
    "\n",
    "    \n",
    "    tmp = list()\n",
    "\n",
    "    for output in encoder_output:\n",
    "        #result = np.array(output.mean(dim=1).tolist())\n",
    "        result = np.array(output.tolist())\n",
    "        \n",
    "        result = np.nan_to_num(result)\n",
    "        #print(output.unsqueeze(0).shape)\n",
    "\n",
    "        tmp.append(result)\n",
    "\n",
    "    #result = encoder_output.mean(dim=0)\n",
    "    #result = np.array(result.mean(dim=1).tolist())\n",
    "    result = np.array(result.tolist())\n",
    "\n",
    "    whole_output.append(result)\n",
    "    sent_output.append(tmp) # input_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b4991fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1570a10f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_output[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e80d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eedd0e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[-0.13549741593579137, -0.009750838162741035, 0.04925700657928063, -0.26253114964476165]\n",
      "max:  2\n",
      "min:  3\n",
      "==============================\n",
      "2\n",
      "[-0.015284969047925917, -0.14575086610476015, -0.06213124984996928, 0.18876781741311366]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "2\n",
      "[-0.016915224187911113, -0.022714634683048664, 0.14550112689872322, -0.06955476481428373]\n",
      "max:  2\n",
      "min:  3\n",
      "==============================\n",
      "0\n",
      "[0.04585616963316472, -0.3681124990644639, -0.018887715909939495, 0.16132834721150566]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "2\n",
      "[-0.0374459648011547, 0.08058759981631222, 0.0072383331623841266, 0.07032142473212843]\n",
      "max:  1\n",
      "min:  0\n",
      "==============================\n",
      "2\n",
      "[-0.042743894687769124, 0.10558521616982305, 0.0893217614561004, 0.09484251921188912]\n",
      "max:  1\n",
      "min:  0\n",
      "==============================\n",
      "3\n",
      "[-0.2830672731523369, -0.3249746014637473, 0.0058415149745423955, -0.058717180829726344]\n",
      "max:  2\n",
      "min:  1\n",
      "==============================\n",
      "2\n",
      "[-0.06355051244712077, 0.05156973077535639, -0.10298011231653405, 0.030725097158494627]\n",
      "max:  1\n",
      "min:  2\n",
      "==============================\n",
      "1\n",
      "[-0.05040129486838699, 0.1431994773874329, 0.07290068297989238, 0.010201196454594736]\n",
      "max:  1\n",
      "min:  0\n",
      "==============================\n",
      "1\n",
      "[0.05116021358996935, -0.07783514669220534, -0.048899000007885746, -0.10714009513494564]\n",
      "max:  0\n",
      "min:  3\n",
      "==============================\n",
      "1\n",
      "[-0.14062842762378153, 0.2003332559904081, 0.15458665190961252, 0.005043026088604656]\n",
      "max:  1\n",
      "min:  0\n",
      "==============================\n",
      "0\n",
      "[-0.02562988238026821, -0.007731994208215709, -0.0669297091605366, -0.10250977634996364]\n",
      "max:  1\n",
      "min:  3\n",
      "==============================\n",
      "2\n",
      "[0.33939749881397624, 0.12764332264036138, 0.056434727229083125, 0.04741326063833743]\n",
      "max:  0\n",
      "min:  3\n",
      "==============================\n",
      "3\n",
      "[-0.11729147893562411, -0.03661603922426803, -0.044497568547536226, 0.07718085203256483]\n",
      "max:  3\n",
      "min:  0\n",
      "==============================\n",
      "0\n",
      "[0.09582498053970066, -0.03348425205080249, -0.16037740796627892, 0.026205872437041294]\n",
      "max:  0\n",
      "min:  2\n",
      "==============================\n",
      "1\n",
      "[0.062368105097687546, -0.09192181527755072, -0.099364868559559, -0.21840897726012706]\n",
      "max:  0\n",
      "min:  3\n",
      "==============================\n",
      "1\n",
      "[0.05725869345540498, 0.030281788927957833, -0.09997367436008622, 0.06933852150507429]\n",
      "max:  3\n",
      "min:  2\n",
      "==============================\n",
      "3\n",
      "[0.016131335094205734, 0.18020951517074948, -0.1105922814983897, -0.005698326071937762]\n",
      "max:  1\n",
      "min:  2\n",
      "==============================\n",
      "0\n",
      "[-0.03941615996000664, -0.05699531288352526, -0.09053716241907225, -0.08322499496308976]\n",
      "max:  0\n",
      "min:  2\n",
      "==============================\n",
      "2\n",
      "[0.1286398373018285, 0.11620427891644543, 0.1845601972166929, 0.022961820964255995]\n",
      "max:  2\n",
      "min:  3\n",
      "==============================\n",
      "2\n",
      "[0.19437288139848208, -0.030245451535565033, -0.02636713068171767, 0.1356704892852094]\n",
      "max:  0\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[0.11766419984749407, 0.2888959846873382, -0.10347887566000999, 0.1601635313981197]\n",
      "max:  1\n",
      "min:  2\n",
      "==============================\n",
      "3\n",
      "[0.04527528701332459, 0.052134933605271265, 0.011669446153020542, 0.004876085327710367]\n",
      "max:  1\n",
      "min:  3\n",
      "==============================\n",
      "3\n",
      "[0.06842915154755806, -0.053040903438259415, 0.07430985747756443, -0.01982055511615486]\n",
      "max:  2\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[-0.2757063793844153, -0.12279526528283738, -0.09645469691574549, -0.12977790452038443]\n",
      "max:  2\n",
      "min:  0\n",
      "==============================\n",
      "2\n",
      "[-0.04106193169004959, 0.07314570936795523, 0.1427001261212735, 0.023507359431416297]\n",
      "max:  2\n",
      "min:  0\n",
      "==============================\n",
      "3\n",
      "[0.11324700399627671, 0.03661701437785352, -0.08878248351203719, 0.010119260935177968]\n",
      "max:  0\n",
      "min:  2\n",
      "==============================\n",
      "3\n",
      "[-0.031096400379424047, 0.22483929068423114, 0.034349382098335275, -0.17439829051892008]\n",
      "max:  1\n",
      "min:  3\n",
      "==============================\n",
      "3\n",
      "[-0.1812458005032187, 0.008589732754638339, 0.08721945691506558, 0.024750322712336438]\n",
      "max:  2\n",
      "min:  0\n",
      "==============================\n",
      "0\n",
      "[-0.02115453415031288, 0.13961359898522618, -0.10638703562428918, -0.09999323068224472]\n",
      "max:  1\n",
      "min:  2\n",
      "==============================\n",
      "2\n",
      "[-0.13121062891271218, -0.15197298289876165, -0.26813674445511476, -0.2624546007452572]\n",
      "max:  0\n",
      "min:  2\n",
      "==============================\n",
      "1\n",
      "[0.2653701875392284, 0.00012992059777662389, -0.12445343798324625, 0.0744443501893686]\n",
      "max:  0\n",
      "min:  2\n",
      "==============================\n",
      "3\n",
      "[0.07054523995861474, 0.003733266391544098, 0.12880697970788607, 0.045254271801153406]\n",
      "max:  2\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[-0.2483167360270521, -0.12276782002124875, -0.19551141038604272, -0.1781664394552465]\n",
      "max:  1\n",
      "min:  0\n",
      "==============================\n",
      "3\n",
      "[-0.00803696438386318, -0.17180223259220642, -0.12309126341665358, 0.09523664730656614]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "0\n",
      "[-0.05412440223079973, 0.03543121558846428, -0.05505262726413584, 0.06292280860975562]\n",
      "max:  3\n",
      "min:  2\n",
      "==============================\n",
      "2\n",
      "[0.06273643020169964, 0.275139753657991, 0.06644877787460876, 0.23863722017607777]\n",
      "max:  1\n",
      "min:  0\n",
      "==============================\n",
      "0\n",
      "[-0.19701049473825566, 0.11549078780387877, -0.17283407373795792, -0.08651286582708653]\n",
      "max:  1\n",
      "min:  0\n",
      "==============================\n",
      "0\n",
      "[0.002090093629590265, -0.09458127137888374, -0.004431237277152687, 0.037393905055772524]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "3\n",
      "[-0.03664241907158087, 0.10919203237202396, -0.043932215527341854, 0.015751359144723743]\n",
      "max:  1\n",
      "min:  2\n",
      "==============================\n",
      "1\n",
      "[0.12902519819885386, 0.14251500918781057, 0.027751089468674064, -0.022411215266339792]\n",
      "max:  1\n",
      "min:  3\n",
      "==============================\n",
      "2\n",
      "[0.15595053764644523, 0.08299606645745747, 0.1034222373402413, -0.10673949013221952]\n",
      "max:  0\n",
      "min:  3\n",
      "==============================\n",
      "0\n",
      "[-0.06432206144542256, 0.1931391753271865, 0.14513552222168258, 0.14259838710729036]\n",
      "max:  1\n",
      "min:  0\n",
      "==============================\n",
      "3\n",
      "[-0.06042076430536843, -0.06064109300838169, -0.2739329979907421, -0.21987578335236974]\n",
      "max:  0\n",
      "min:  2\n",
      "==============================\n",
      "3\n",
      "[-0.19176961172264337, -0.06316066445920081, -0.04246032902744212, -0.10084769842949431]\n",
      "max:  2\n",
      "min:  0\n",
      "==============================\n",
      "3\n",
      "[-0.27204994896437373, -0.0918715094357557, -0.06383557572598665, -0.06750483440975621]\n",
      "max:  2\n",
      "min:  0\n",
      "==============================\n",
      "3\n",
      "[0.07826436670799733, 0.2217020874057578, -0.11670562079088155, 0.09498852108366224]\n",
      "max:  1\n",
      "min:  2\n",
      "==============================\n",
      "2\n",
      "[0.027991977320865804, 0.12389911315156693, -0.0006530102409592169, 0.037252122792719236]\n",
      "max:  1\n",
      "min:  2\n",
      "==============================\n",
      "3\n",
      "[0.05377908119839107, 0.03729524301286062, -0.08486811400381505, -0.06180380692429819]\n",
      "max:  0\n",
      "min:  2\n",
      "==============================\n",
      "2\n",
      "[-0.1406183597952223, 0.010798302639121413, -0.013626357063858554, 0.12568138676585358]\n",
      "max:  3\n",
      "min:  0\n",
      "==============================\n",
      "2\n",
      "[0.012376242725506427, 0.114886881656847, -0.03453126453458864, 0.17606880008735432]\n",
      "max:  3\n",
      "min:  2\n",
      "==============================\n",
      "1\n",
      "[0.05262733722078551, 0.0813994954343803, 0.0919632610777655, 0.24532361484586776]\n",
      "max:  3\n",
      "min:  0\n",
      "==============================\n",
      "2\n",
      "[0.10033480973738362, -0.2295063812739547, 0.08309912717318423, -0.19537869126111182]\n",
      "max:  0\n",
      "min:  1\n",
      "==============================\n",
      "2\n",
      "[-0.03355078923774697, 0.11524642570349138, 0.09761825895433593, -0.21771334312321008]\n",
      "max:  1\n",
      "min:  3\n",
      "==============================\n",
      "0\n",
      "[0.041436881349491014, 0.19444387494534904, 0.19092207560837174, 0.1032106525252628]\n",
      "max:  1\n",
      "min:  0\n",
      "==============================\n",
      "0\n",
      "[0.07988087921454444, 0.12163568465631971, 0.05040775574610725, -0.19608131025070957]\n",
      "max:  1\n",
      "min:  3\n",
      "==============================\n",
      "2\n",
      "[0.0064973917420365155, -0.03381979236674208, -0.017948880685447706, -0.05907877669834124]\n",
      "max:  0\n",
      "min:  3\n",
      "==============================\n",
      "2\n",
      "[-0.06575838652350995, -0.07707463730856, 0.11439677003435235, 0.17006118545694773]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[-0.04834617263113127, -0.18311437241028117, -0.30846434671586076, 0.002116231082677381]\n",
      "max:  3\n",
      "min:  2\n",
      "==============================\n",
      "1\n",
      "[0.3271536814624239, -0.026338759940001306, -0.1034634454168571, 0.04252287042319856]\n",
      "max:  0\n",
      "min:  2\n",
      "==============================\n",
      "0\n",
      "[-0.09896704625337434, 0.0802551631343664, 0.007598384636243048, 0.17914622548796885]\n",
      "max:  3\n",
      "min:  0\n",
      "==============================\n",
      "3\n",
      "[-0.10038648923414663, -0.08781219210005639, 0.11791345901653769, -0.011565417923473159]\n",
      "max:  2\n",
      "min:  0\n",
      "==============================\n",
      "2\n",
      "[-0.040063903474572365, 0.09724174730592279, 0.09998698685299051, 0.2535074531747622]\n",
      "max:  3\n",
      "min:  0\n",
      "==============================\n",
      "0\n",
      "[-0.08366545790368957, -0.07084020535364227, -0.06509408898919739, -0.11519803692280287]\n",
      "max:  2\n",
      "min:  3\n",
      "==============================\n",
      "1\n",
      "[0.17455226269891203, 0.20792285403785918, -0.12318468391808497, -0.1037593025093045]\n",
      "max:  1\n",
      "min:  2\n",
      "==============================\n",
      "1\n",
      "[0.08194637582640739, 0.06238348390507593, -0.17384998544282645, 0.10464547486844655]\n",
      "max:  3\n",
      "min:  2\n",
      "==============================\n",
      "0\n",
      "[0.08204225346432051, -0.045305428978222685, 0.07245441042583546, 0.19405057199000203]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "0\n",
      "[0.07650699804726664, -0.02026934661362292, 0.05847110537471277, 0.14749924458662325]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[0.031118898313577947, 0.12558597183977743, -0.054509675688392246, -0.13149588029813808]\n",
      "max:  1\n",
      "min:  3\n",
      "==============================\n",
      "1\n",
      "[0.23546163111589147, 0.02613732001584969, 0.23220699768206665, -0.017087245292780766]\n",
      "max:  0\n",
      "min:  3\n",
      "==============================\n",
      "0\n",
      "[0.06566518237344196, -0.08819688530542114, 0.07004687765221196, 0.05361607557252824]\n",
      "max:  2\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[0.035250549751548045, -0.06640146802082475, -0.057162527802287984, 0.017137828123604423]\n",
      "max:  0\n",
      "min:  1\n",
      "==============================\n",
      "0\n",
      "[0.10858345335357948, 0.08552164116230991, -0.006008163190214565, 0.05047647126401678]\n",
      "max:  0\n",
      "min:  2\n",
      "==============================\n",
      "0\n",
      "[-0.024131463997298746, -0.07553565328156106, 0.0007271507791845334, 0.014631819004824523]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[-0.2225815939924006, -0.2644053860815237, 0.0032275678432937033, 0.043440758029929805]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "2\n",
      "[-0.045707582794460466, 0.19260363334564784, 0.13407359783518802, -0.07139852682475077]\n",
      "max:  1\n",
      "min:  3\n",
      "==============================\n",
      "3\n",
      "[-0.1291445981553204, 0.0008864614225361403, -0.08645972179731075, 0.002110343067070057]\n",
      "max:  3\n",
      "min:  0\n",
      "==============================\n",
      "0\n",
      "[0.08280228092687289, -0.21010016481238758, -0.04188638133040756, 0.05465357525891168]\n",
      "max:  0\n",
      "min:  1\n",
      "==============================\n",
      "3\n",
      "[0.2508940849975075, -0.15799043288312586, 0.04598834184265879, 0.059082063808120755]\n",
      "max:  0\n",
      "min:  1\n",
      "==============================\n",
      "3\n",
      "[-0.08311057471350244, 0.10931640785974213, 0.14172079332819856, -0.08019020495454071]\n",
      "max:  2\n",
      "min:  0\n",
      "==============================\n",
      "2\n",
      "[-0.04372737252970932, -0.12159647708462674, 0.11384631272703878, 0.10409660859657141]\n",
      "max:  2\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[-0.17352203124606796, -0.024528768745704043, -0.27731786012306553, 0.07822732408238824]\n",
      "max:  3\n",
      "min:  2\n",
      "==============================\n",
      "1\n",
      "[0.023830559285998094, -0.028450550230444457, 0.03212255731149356, -0.19245401826326236]\n",
      "max:  2\n",
      "min:  3\n",
      "==============================\n",
      "3\n",
      "[0.14335922837506299, 0.10819267457959623, 0.10435731021158698, 0.0742959138649683]\n",
      "max:  0\n",
      "min:  3\n",
      "==============================\n",
      "1\n",
      "[-0.09628886401272434, -0.018744580605154014, -0.030873749627859864, -0.003140781125486027]\n",
      "max:  3\n",
      "min:  0\n",
      "==============================\n",
      "3\n",
      "[0.057670339527163335, 0.050956300848642326, -0.4135129698967551, 0.03174600733156811]\n",
      "max:  0\n",
      "min:  2\n",
      "==============================\n",
      "0\n",
      "[-0.22463832625643002, 0.14952235234550784, -0.11919811087572556, -0.18467211344967546]\n",
      "max:  1\n",
      "min:  0\n",
      "==============================\n",
      "3\n",
      "[-0.16688610577992757, -0.3505830621695217, 0.036394103354454026, 0.13243264640531024]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[0.07431771033474938, 0.03776166203280552, 0.1304379039054898, 0.1411846299569196]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[0.24101608218971218, -0.06619454340350915, 0.10372013406315793, -0.0035085875148373807]\n",
      "max:  0\n",
      "min:  1\n",
      "==============================\n",
      "0\n",
      "[0.12502101023444692, -0.05974850759233414, 0.0006521008598727375, 0.15423556206780417]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "2\n",
      "[-0.17917976344680603, 0.19094285103537145, 0.09048000222452364, -0.01778719533348677]\n",
      "max:  1\n",
      "min:  0\n",
      "==============================\n",
      "2\n",
      "[0.03674119919710907, 0.04548856604273332, 0.045657399447795004, 0.04313868003456743]\n",
      "max:  2\n",
      "min:  0\n",
      "==============================\n",
      "3\n",
      "[0.010104685068658466, 0.2758878436717459, -0.057905253256413124, -0.043913243032811154]\n",
      "max:  1\n",
      "min:  2\n",
      "==============================\n",
      "2\n",
      "[-0.11498659352909983, 0.08484132641707087, 0.12407748726505655, -0.03835232888420424]\n",
      "max:  2\n",
      "min:  0\n",
      "==============================\n",
      "0\n",
      "[-0.024896763349078716, -0.19256845142196607, -0.16974500781653332, 0.10815139032500466]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "0\n",
      "[0.2108467174765067, -0.03352324102264532, 0.1859573441001448, 0.006972898592183853]\n",
      "max:  0\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[0.043322791954095316, -0.27009403811051264, 0.04527539081787167, 0.18017955760904586]\n",
      "max:  3\n",
      "min:  1\n",
      "==============================\n",
      "1\n",
      "[0.10569291980690972, 0.08870781005764111, -0.0348055184712686, 0.03799605690833443]\n",
      "max:  0\n",
      "min:  2\n",
      "==============================\n",
      "0\n",
      "[0.13013669748202494, 0.00013354085497212852, 0.056813014122500734, -0.04414971821182278]\n",
      "max:  0\n",
      "min:  3\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for n, sent in enumerate(sent_output):\n",
    "    tmp = list()\n",
    "    labels = examples[n]['labels'].split(\"\\n\")\n",
    "    answer = labels.index(\"1\")\n",
    "    print(answer)\n",
    "    for i in sent:\n",
    "        tmp.append(cos_sim(i, whole_output[n]))\n",
    "    maxcosine = tmp.index(min(tmp))\n",
    "    print(tmp)\n",
    "    if answer == maxcosine:\n",
    "        cnt += 1\n",
    "    print(\"max: \", tmp.index(max(tmp)))\n",
    "    print(\"min: \", tmp.index(min(tmp)))\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c88c17e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33264a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def dist(x,y):   \n",
    "    return np.sqrt(np.sum((x-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4e32eb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,) (100,256) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-58f4935297cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhole_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmaxcosine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-5323a542102f>\u001b[0m in \u001b[0;36mdist\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,) (100,256) "
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for n, sent in enumerate(sent_output):\n",
    "    tmp = list()\n",
    "    labels = examples[n]['labels'].split(\"\\n\")\n",
    "    answer = labels.index(\"1\")\n",
    "    print(answer)\n",
    "    for i in sent:\n",
    "        tmp.append(dist(i, whole_output[n]))\n",
    "    maxcosine = tmp.index(max(tmp))\n",
    "    print(tmp)\n",
    "    if answer == maxcosine:\n",
    "        cnt += 1\n",
    "    print(\"max: \", tmp.index(max(tmp)))\n",
    "    print(\"min: \", tmp.index(min(tmp)))\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d5223623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "26548e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = cos(whole_output[0], sent_output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1663650e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "        [ 0.8218, -0.6166,  0.9100,  ..., -0.2476,  0.6131,  0.0065],\n",
       "        [ 0.8420,  0.4001, -0.4663,  ..., -0.1339,  0.5475,  0.0224],\n",
       "        [ 0.2929,  0.6657,  0.7336,  ..., -0.0712,  0.8906, -0.0017]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1abfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
